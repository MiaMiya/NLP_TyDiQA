{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30575,"status":"ok","timestamp":1669644474012,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"vri4tKs7KQ0I","outputId":"cf888c91-ac49-41a4-b9c0-518ab177c5be"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting bpemb\n","  Using cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n","Requirement already satisfied: gensim in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from bpemb) (4.1.2)\n","Requirement already satisfied: tqdm in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from bpemb) (4.62.2)\n","Collecting sentencepiece\n","  Using cached sentencepiece-0.1.97-cp37-cp37m-win_amd64.whl (1.1 MB)\n","Requirement already satisfied: numpy in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from bpemb) (1.19.2)\n","Requirement already satisfied: requests in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from bpemb) (2.26.0)\n","Requirement already satisfied: Cython==0.29.23 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from gensim->bpemb) (0.29.23)\n","Requirement already satisfied: scipy>=0.18.1 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from gensim->bpemb) (1.7.1)\n","Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from gensim->bpemb) (5.2.1)\n","Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from tqdm->bpemb) (0.4.4)\n","Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests->bpemb) (3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests->bpemb) (2021.5.30)\n","Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests->bpemb) (2.0.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests->bpemb) (1.26.6)\n","Installing collected packages: sentencepiece, bpemb\n","Successfully installed bpemb-0.3.4 sentencepiece-0.1.97\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: You are using pip version 20.1.1; however, version 22.3.1 is available.\n","You should consider upgrading via the 'c:\\program files\\python37\\python.exe -m pip install --upgrade pip' command.\n"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (1.19.2)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (5.4.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (4.8.1)"]},{"name":"stderr","output_type":"stream","text":["  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\miaha\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\miaha\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","WARNING: You are using pip version 20.1.1; however, version 22.3.1 is available.\n","You should consider upgrading via the 'c:\\program files\\python37\\python.exe -m pip install --upgrade pip' command.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (4.62.2)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-win_amd64.whl (3.3 MB)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (21.0)\n","Collecting filelock\n","  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: requests in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (2.26.0)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (2021.9.30)\n","Requirement already satisfied: zipp>=0.5 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.10.0.0)\n","Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests->transformers) (3.2)\n","Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests->transformers) (1.26.6)\n","Installing collected packages: filelock, huggingface-hub, tokenizers, transformers\n","Successfully installed filelock-3.9.0 huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n","Defaulting to user installation because normal site-packages is not writeable"]},{"name":"stderr","output_type":"stream","text":["  WARNING: The script plasma_store.exe is installed in 'C:\\Users\\miaha\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script datasets-cli.exe is installed in 'C:\\Users\\miaha\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","WARNING: You are using pip version 20.1.1; however, version 22.3.1 is available.\n","You should consider upgrading via the 'c:\\program files\\python37\\python.exe -m pip install --upgrade pip' command.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Collecting datasets\n","  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n","Collecting pyarrow>=6.0.0\n","  Downloading pyarrow-10.0.1-cp37-cp37m-win_amd64.whl (20.2 MB)\n","Collecting fsspec[http]>=2021.11.1\n","  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from datasets) (0.11.1)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from datasets) (1.19.2)\n","Requirement already satisfied: aiohttp in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from datasets) (3.7.4.post0)\n","Requirement already satisfied: requests>=2.19.0 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from datasets) (2.26.0)\n","Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from datasets) (4.62.2)\n","Collecting multiprocess\n","  Using cached multiprocess-0.70.14-py37-none-any.whl (115 kB)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp37-cp37m-win_amd64.whl (30 kB)\n","Collecting responses<0.19\n","  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from datasets) (5.4.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from datasets) (4.8.1)\n","Requirement already satisfied: packaging in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from datasets) (21.0)\n","Requirement already satisfied: pandas in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from datasets) (1.3.5)\n","Collecting dill<0.3.7\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0.0)\n","Requirement already satisfied: filelock in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n","Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from aiohttp->datasets) (4.0.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from aiohttp->datasets) (5.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from aiohttp->datasets) (3.0.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests>=2.19.0->datasets) (3.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests>=2.19.0->datasets) (1.26.6)\n","Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n","Requirement already satisfied: zipp>=0.5 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.5.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: pytz>=2017.3 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from pandas->datasets) (2021.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in c:\\users\\miaha\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: pyarrow, fsspec, dill, multiprocess, xxhash, responses, datasets\n","Successfully installed datasets-2.8.0 dill-0.3.6 fsspec-2022.11.0 multiprocess-0.70.14 pyarrow-10.0.1 responses-0.18.0 xxhash-3.2.0\n"]}],"source":["!pip install bpemb\n","!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5784,"status":"ok","timestamp":1669644479790,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"PZ0ZcuAgKUFC","outputId":"95d36eca-c24a-47b3-b5ff-a603152e6567"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\miaha\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\miaha\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","#from turtle import write_docstringdict\n","from nltk.tokenize import (word_tokenize,TreebankWordTokenizer)\n","import nltk\n","from nltk.corpus import stopwords\n","\n","import numpy as np\n","from numpy import array\n","from numpy import argmax\n","\n","import pandas as pd\n","\n","#import pyarrow as pa\n","#import pyarrow.parquet as pq\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","pd.options.mode.chained_assignment = None\n","\n","import pickle\n","\n","from pathlib import Path\n","\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","torch.manual_seed(123)\n","from torch.utils.data import Dataset,DataLoader\n","#from torch.utils.data import Dataset, DataLoader\n","from torch import nn\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import ExponentialLR, CyclicLR \n","from torch.optim.lr_scheduler import LambdaLR\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn import metrics\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import confusion_matrix\n","\n","from tqdm import tqdm, tqdm_notebook\n","from tqdm.notebook import tqdm\n","\n","from bpemb import BPEmb\n","#from .autonotebook import tqdm as notebook_tqdm\n","\n","import transformers\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n","from transformers import Trainer, TrainingArguments\n","from transformers import PreTrainedTokenizer\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import AutoModelForQuestionAnswering\n","from transformers import AutoConfig\n","\n","import datasets\n","from datasets import DatasetDict, Dataset, load_dataset, load_metric\n","\n","import math\n","from math import log\n","\n","from typing import List, Tuple, AnyStr\n","\n","import string\n","\n","import io\n","\n","import random\n","\n","#from torchcrf import CRF\n","\n","import matplotlib.pyplot as plt\n","\n","from copy import deepcopy\n","\n","import heapq\n","\n","from functools import partial\n","\n","from collections import defaultdict, OrderedDict\n","\n","import re\n","\n","%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1669644479791,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"yPMerxrXKYZ-","outputId":"4caa3b59-7d77-44ea-a622-b44c5a13d06c"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device(\"cpu\")\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","print(device)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6400,"status":"ok","timestamp":1669644486184,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"kjaGj3M3M51t","outputId":"1ce35685-1269-4e61-da61-263dbfd6eff1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# For using colab\n","from google.colab import drive\n","drive.mount('/content/drive') # this will trigger permission prompts\n","path = '/content/drive/MyDrive/Deep Learning - NLP project/'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We have in total 6 pairs models (12 in total). For the following document one set of models can be selected and run. Since the procedures are for each model are identical just rerun the code with another pair of models. This would require restarting the kernel/clearning the memory since the training the models will use up allmost all memeory. "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":361,"status":"ok","timestamp":1669644499417,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"wrdflF6xLJVN","outputId":"2441c940-5cb9-44db-d716-39c2a8692fb2"},"outputs":[{"data":{"text/plain":["('model_classification_m_roberta_en.pt', 'model_extract_m_roberta_en.pt')"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Selecting models\n","all_models = {'model_classification_BERT_en.pt':'bert-base-cased', 'model_extract_BERT_en.pt':'bert-base-cased',\n","        'model_classification_m_BERT_en.pt':'bert-base-multilingual-cased', 'model_extract_m_BERT_en.pt':'bert-base-multilingual-cased',\n","        'model_classification_m_BERT_ko.pt':'bert-base-multilingual-cased', 'model_extract_m_BERT_ko.pt': 'bert-base-multilingual-cased',\n","        'model_classification_roberta_en.pt':'roberta-base',   'model_extract_roberta_en.pt':'roberta-base', \n","        'model_classification_m_roberta_en.pt':'xlm-roberta-base',  'model_extract_m_roberta_en.pt':'xlm-roberta-base',\n","        'model_classification_m_roberta_ko.pt':'xlm-roberta-base',  'model_extract_m_roberta_ko.pt':'xlm-roberta-base'}\n","model_name_class = list(all_models.keys())[8]\n","model_name_extract = list(all_models.keys())[9]\n","model_name_class, model_name_extract"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The next step is to load the tokenizer from the belonging model. The fine-tuned models will be loaded, this is for evaluation of the models without needing to fine-tune them again. "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4454,"status":"error","timestamp":1669645694967,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"_kc0BnfuLVlg","outputId":"0b119fa9-086f-45aa-a297-1b2499bffb61"},"outputs":[{"data":{"text/plain":["3"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Load Tokenzier from huggingface\n","tokenizer = AutoTokenizer.from_pretrained(all_models[model_name_class])\n","tokenizer.add_special_tokens({'bos_token': '|CLS|',\n","                                'eos_token': '|SEP|',\n","                                'pad_token': '<pad>',\n","                                'unk_token': '|UNK|'})\n","\n","# Load fine-tuned models\n","model_class = torch.load(path + model_name_class).to(device)\n","model_extract = torch.load(path + model_name_extract).to(device)\n","model_extract.resize_token_embeddings(len(tokenizer))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Preprocess data "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before we start working with our models we need to prepare our data. We are using the [TyDi QA](https://huggingface.co/datasets/copenlu/answerable_tydiqa) dataset, before we start fine-tuning the model we will convert the data into SQuAD format. "]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":488,"status":"ok","timestamp":1669644512340,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"h10Rg_MTKbnN"},"outputs":[],"source":["def preprocess(df1, df2 = None, exist = False, lan = 'english'):\n","\n","    df_train = pd.concat([df1.drop(['annotations'], axis=1), df1['annotations'].apply(pd.Series)], axis=1)\n","\n","    ## label 1 for answerable questions and 0 for unanswerable questions\n","    df_train['label'] = [1 if x != [-1] else 0 for x in df_train['answer_start'].values]\n","    \n","    ## select relevant feature columns \n","    cols = ['question_text','document_plaintext','label','answer_start','answer_text']\n","    df_train = df_train[cols]\n","      \n","    df_train['text'] = df_train[\"question_text\"] + \" |SEP| \" + df_train['document_plaintext'] \n"," \n","    if exist: \n","        df2[[\"answer_start_1\",\"answer_text\"]] = df2['annotations'].apply(pd.Series)\n","\n","        df_val = pd.concat([df2.drop(['annotations'], axis=1), df2['annotations'].apply(pd.Series)], axis=1)\n","\n","        ## label 1 for answerable questions and 0 for unanswerable questions\n","        df_val['label'] = [1 if x != [-1] else 0 for x in df_val['answer_start'].values]\n","\n","        ## select relevant feature columns \n","        df_val = df_val[cols]\n","\n","        df_val['text'] = df_val[\"question_text\"] + \" |SEP| \" + df_val['document_plaintext']\n","\n","    if exist:\n","      return df_train.sample(frac=1).reset_index(drop=True), df_val.reset_index(drop=True)\n","    else:\n","      return df_train.sample(frac=1).reset_index(drop=True)\n","\n","def create_squad(df_train, df_val):\n","\n","    df_train_squad = df_train[df_train['label'] == 1].reset_index(drop=True)\n","    df_train_squad_full = df_train\n","    df_val_squad = df_val[df_val['label'] == 1].reset_index(drop=True)\n","    df_val_squad_full = df_val\n","\n","    l_d_answers_t = []\n","    l_d_id_t = []\n","    for index, row in df_train_squad.iterrows():\n","        l_d_answers_t.append({'answer_start':row['answer_start'], 'text': row['answer_text']})\n","        l_d_id_t.append(f'THIS_IS_{index}_TEXT_IN_TRAIN')\n","\n","    l_d_answers_t_f = []\n","    l_d_id_t_f = []\n","    for index, row in df_train_squad_full.iterrows():\n","        l_d_answers_t_f.append({'answer_start':row['answer_start'], 'text': row['answer_text']})\n","        l_d_id_t_f.append(f'THIS_IS_{index}_TEXT_IN_TRAIN')    \n","\n","    l_d_answers_v = []\n","    l_d_id_v = []\n","    for index, row in df_val_squad.iterrows():\n","        l_d_answers_v.append({'answer_start':row['answer_start'], 'text': row['answer_text'][0]}) # Not sure why they are in an extra list\n","        l_d_id_v.append(f'THIS_IS_{index}_TEXT_IN_VAL_ANSWERABLE')\n","\n","    l_d_answers_f = []\n","    l_d_id_v_f = []\n","    for index, row in df_val_squad_full.iterrows():\n","        l_d_answers_f.append({'answer_start':row['answer_start'], 'text': row['answer_text'][0]}) # Not sure why they are in an extra list\n","        l_d_id_v_f.append(f'THIS_IS_{index}_TEXT_IN_VAL_FULL')\n","\n","    train_squad = pd.DataFrame({'answers':l_d_answers_t, 'context': df_train_squad['document_plaintext'], \n","                        'question':df_train_squad['question_text'],'id': l_d_id_t})\n","    \n","    train_squad_full = pd.DataFrame({'answers':l_d_answers_t_f, 'context': df_train_squad_full['document_plaintext'], \n","                        'question':df_train_squad_full['question_text'],'id': l_d_id_t_f})\n","    \n","    val_squad = pd.DataFrame({'answers':l_d_answers_v, 'context': df_val_squad['document_plaintext'], \n","                        'question':df_val_squad['question_text'],'id': l_d_id_v})\n","    \n","    val_full_squad = pd.DataFrame({'answers':l_d_answers_f, 'context': df_val_squad_full['document_plaintext'], \n","                        'question':df_val_squad_full['question_text'],'id': l_d_id_v_f})\n","    \n","    train_dataset_squad = Dataset.from_pandas(train_squad)\n","    train_dataset_squad_full = Dataset.from_pandas(train_squad_full)\n","    test_dataset_squad = Dataset.from_pandas(val_squad)\n","    full_test_dataset_squad = Dataset.from_pandas(val_full_squad)\n","\n","    return datasets.DatasetDict({\"train\":train_dataset_squad, \"train_full\":train_dataset_squad_full,\"validation\":test_dataset_squad, \"validation_full\":full_test_dataset_squad})\n","\n","def question_parag_combine(questions, paragraphs):\n","    \"\"\"\n","    This function combines the questions and paragraphs into a single text\n","    Args:\n","        questions: list of questions\n","        paragraphs: list of paragraphs\n","    Returns:\n","        list of combined questions and paragraphs\n","    \"\"\"\n","    training_data = []\n","    for index in range(len(questions)):\n","        training_data += [\"|CLS| \" + questions[index] + \" |sep| \" + paragraphs[index] + \" |sep|\"]\n","        \n","    return training_data\n","\n","def load_data(df_train=None, df_val=None, path_to_file=None):\n","  if path_to_file is None or not Path(path_to_file).is_file():\n","    df_en_train, df_en_val = preprocess(df_train[df_train['language'] == 'english'], df2 = df_val[df_val['language'] == 'english'],exist = True)\n","\n","    df_ko_train, df_ko_val = preprocess(df_train[df_train['language'] == 'korean'], df2 = df_val[df_val['language'] == 'korean'],exist = True, lan = 'korean')\n","    \n","    df_lan_dict = {'english': \n","    {'train': df_en_train, 'val': df_en_val},\n","    'korean': \n","    {'train': df_ko_train, 'val': df_ko_val}\n","    }\n","\n","    df_lan_dict['english']['df_train'] = df_en_train\n","    df_lan_dict['english']['df_val'] = df_en_val\n","\n","    df_lan_dict['korean']['df_train'] = df_ko_train\n","    df_lan_dict['korean']['df_val'] = df_ko_val\n","\n","    #questions mark??? vvv\n","    df_en_train = df_en_train.reset_index().drop(columns='index')\n","    df_en_val = df_en_val.reset_index().drop(columns='index')\n","\n","    df_ja_train = df_ko_train.reset_index().drop(columns='index')\n","    df_ja_val = df_ko_val.reset_index().drop(columns='index')\n","\n","    df_lan_dict['english']['train'] = df_en_train\n","    df_lan_dict['english']['val'] = df_en_val\n","\n","    df_lan_dict['korean']['train'] = df_ko_train\n","    df_lan_dict['korean']['val'] = df_ko_val\n","\n","    for lan in ['english', 'korean']:\n","      df_lan_dict[lan]['train_dataset'] = Dataset.from_pandas(df_lan_dict[lan]['train'][['question_text','document_plaintext','label']])\n","      df_lan_dict[lan]['test_dataset'] = Dataset.from_pandas(df_lan_dict[lan]['val'][['question_text','document_plaintext','label']])\n","      df_lan_dict[lan]['dict_dataset'] = datasets.DatasetDict({\"train\": df_lan_dict[lan]['train_dataset'], \"test\": df_lan_dict[lan]['test_dataset']})\n","\n","      d_q = df_lan_dict[lan]['train_dataset']['question_text']\n","      d_p = df_lan_dict[lan]['train_dataset']['document_plaintext']\n","      training_data = question_parag_combine(d_q, d_p)\n","      training_labels =  df_lan_dict[lan]['train_dataset']['label']\n","\n","      d_q = df_lan_dict[lan]['test_dataset']['question_text']\n","      d_p = df_lan_dict[lan]['test_dataset']['document_plaintext']\n","      validation_data = question_parag_combine(d_q, d_p)\n","      validation_labels = df_lan_dict[lan]['test_dataset']['label']\n","\n","      data_set = {}\n","      sets = [['train',training_data, training_labels], ['val',validation_data, validation_labels]]\n","      for meta in sets:\n","          data_set[meta[0]] = {}\n","          data_set[meta[0]]['text'] = []\n","          data_set[meta[0]]['label'] = []\n","          \n","          for ind, text in enumerate(meta[1]):\n","              data_set[meta[0]]['text'].append(text)\n","              data_set[meta[0]]['label'].append(meta[2][ind])\n","              \n","      data_set = DatasetDict({'train':Dataset.from_dict(data_set['train']),\n","                              'valid':Dataset.from_dict(data_set['val'])\\\n","                          })\n","      df_lan_dict[lan]['data_set'] = data_set\n","    \n","    df_lan_dict['english']['train']['text'] = \"|CLS| \" + df_en_train[\"question_text\"] + \" |sep| \" + df_en_train['document_plaintext'] + \" |sep|\"\n","    df_lan_dict['english']['val']['text'] = \"|CLS| \" + df_en_val[\"question_text\"] + \" |sep| \" + df_en_val['document_plaintext'] + \" |sep|\"\n","\n","    df_lan_dict['korean']['train']['text'] = \"|CLS| \" + df_ko_train[\"question_text\"] + \" |sep| \" + df_ko_train['document_plaintext'] + \" |sep|\"\n","    df_lan_dict['korean']['val']['text'] = \"|CLS| \" + df_ko_val[\"question_text\"] + \" |sep| \" + df_ko_val['document_plaintext'] + \" |sep|\"\n","\n","    with open(path + 'df_lan_dict.pickle', 'wb') as handle:\n","      pickle.dump(df_lan_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    \n","    with open(path + 'df_lan_dict.pickle', 'rb') as handle:\n","      df_lan_dict = pickle.load(handle)\n","    \n","    return df_lan_dict\n","  else:\n","    with open(path_to_file, 'rb') as handle:\n","      df_lan_dict = pickle.load(handle)\n","    \n","    return df_lan_dict\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6214,"status":"ok","timestamp":1669644522024,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"TdkdDeiSKgFK"},"outputs":[],"source":["## training set\n","df_train = pd.read_parquet(path + 'train-00000-of-00001-af2f3eaa87d1aa8b.parquet')\n","\n","## validation set \n","df_val = pd.read_parquet(path + 'validation-00000-of-00001-1f04eb244a33fa1b.parquet')\n","\n","# Get english subset\n","df_en_train, df_en_val = preprocess(df_train[df_train['language'] == 'english'],\n","                                        df2 = df_val[df_val['language'] == 'english'], exist = True, lan = 'english')\n","en_dataset_squad = create_squad(df_en_train, df_en_val)\n","\n","# Get korean subset\n","df_ko_train, df_ko_val = preprocess(df_train[df_train['language'] == 'korean'],\n","                                        df2 = df_val[df_val['language'] == 'korean'], exist = True, lan = 'korean')\n","ko_dataset_squad = create_squad(df_ko_train, df_ko_val)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1669644613290,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"fmWThPVgjhsR","outputId":"c1a1e30d-6f1d-490c-abf9-7be8138cf354"},"outputs":[{"data":{"text/plain":["label\n","0    309\n","1    308\n","Name: text, dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["df_ko_val.groupby('label').count()['text']"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669644616269,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"vb-_wL0COBVR","outputId":"3ef8a8aa-5723-4554-81e8-c15261752c5c"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['answers', 'context', 'question', 'id'],\n","        num_rows: 1625\n","    })\n","    train_full: Dataset({\n","        features: ['answers', 'context', 'question', 'id'],\n","        num_rows: 3249\n","    })\n","    validation: Dataset({\n","        features: ['answers', 'context', 'question', 'id'],\n","        num_rows: 308\n","    })\n","    validation_full: Dataset({\n","        features: ['answers', 'context', 'question', 'id'],\n","        num_rows: 617\n","    })\n","})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["ko_dataset_squad"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669644618031,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"1YUqXa8HZbQP"},"outputs":[],"source":["# Load another preprocessed data which in a dataframe format. \n","with open(path+'df_lan_dict.pickle', 'rb') as handle:\n","  df_lan_dict = pickle.load(handle)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Classification model "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yiomxInlnQEF"},"source":["### Fine-tune classification model "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["First we define our metrics which the model will be evaluted on we have selected f1-score. "]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":288,"status":"ok","timestamp":1669644636104,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"60_sBxPJZcun"},"outputs":[],"source":["from datasets import load_metric\n","metric = load_metric('f1')\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions.flatten(), references=labels.flatten(),average='weighted')"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":431,"status":"ok","timestamp":1669644695293,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"X5CvRd7uZeYk"},"outputs":[],"source":["# Define the training procedure\n","def model_gpt2_train(dict, lan, n_epochs=1, model_gpt2 = None):\n","\n","    # Load model form huggingface if no input model found\n","    if model_gpt2 is None:\n","        \n","        model_gpt2 = AutoModelForSequenceClassification.from_pretrained(all_models[model_name_class], num_labels = 2).to(device)\n","        model_gpt2.resize_token_embeddings(len(tokenizer))\n","    \n","    def tokenize_function(examples):\n","      return tokenizer(examples[\"text\"], max_length = 512, padding='max_length',truncation=True)\n","\n","    tydiqa_tok_splits = dict[lan]['data_set']\n","\n","    tydiqa_tok_lm = tydiqa_tok_splits.map(tokenize_function, batched=True)\n","\n","    # Select training arguments\n","    training_args = TrainingArguments(\n","        \"test-clm\",\n","        evaluation_strategy = \"epoch\",\n","        learning_rate=2e-5,\n","        weight_decay=0.01,\n","        num_train_epochs=n_epochs\n","    )\n","\n","    # Define trainer\n","    trainer = Trainer(\n","        model=model_gpt2,\n","        args=training_args,\n","        train_dataset=tydiqa_tok_lm['train'],\n","        eval_dataset=tydiqa_tok_lm['valid'],\n","        compute_metrics=compute_metrics)\n","\n","    eval_results = trainer.evaluate()\n","    print(f\"Perplexity before training: {math.exp(eval_results['eval_loss']):.2f}\")\n","\n","    # Training the model\n","    trainer.train()\n","\n","    eval_results = trainer.evaluate()\n","\n","    print(f\"Perplexity after training: {math.exp(eval_results['eval_loss']):.2f}\")\n","    path_file = path + 'model_classification_m_roberta_' + lan[0:2] + '.pt'\n","\n","    # Saving the model \n","    torch.save(model_gpt2, path_file)\n","    return torch.load(path_file)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2ed96f21e38841b88db2d9428406e50e","f92b95b91ef74de68b960625d918bc11","09009fc3c3c344c2b08e2df9b0e87e1c","6c7ea21ddf9f459fabaeb31d62d6cb93","f8f1912d88ed4198b7925dde37f3ad3b","1334c3ecc63c42a9bf779fb673f965d5","05c6f06505e54e829c5afc18f07af47d","fd36d143a46d4c0dae8f46ac7b97fa3a","89e1a06a857a4dcbbdadbdabc529a0a4","f28e4c381bde408c899e876a5d3075c8","f49912a08ea7433ba389e920afdee97d","bc89a2051096408480e72fa0aa5f9fa9","6787addb441b420b834d48d087a58204","d17c2e3ddf0e441885981364424f5734","35e655cd5fe24be5886af36048da6314","be0dc8d109364014997578f7eafe0aa7","02bf58e376f5429a9af749cc5853ee24","9a99022734fc4112a647037702c458c7","c25ec103e1ba4d30bc2066afd3530d78","c3fef546180940688390903fd2a29e86","60c42d13b8c44b3f82c8133ae6d4b281","93c468a4823946469fe276bb0ab7c29c","88dad87f69d143b9accc86ca86563927","da0642f7ca914715b2aced4dd1db3ae4","ccf9e60775e2400abb582801dff0d3cb","580dca12a6a645448981a1bce967cea1","5c153fb69e25484fa78d47ec41df9ca7","44d1540a40534b309843b31fc401420c","c8576626ae9342a580941ae95e30c3e5","3016f7fb12ab4822923c0bd90d1e7e44","4f1921cbe3fb46c59090c640d4103a0a","40228fafb15b49d193df1df3e68d6c9a","8022db8e1d4c4fe190acefe3d0ac54a4"]},"executionInfo":{"elapsed":973620,"status":"ok","timestamp":1669645673007,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"tOjOm7LDZg6N","outputId":"a836f207-d800-41df-a7fc-ae73fa894398"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ed96f21e38841b88db2d9428406e50e","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc89a2051096408480e72fa0aa5f9fa9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88dad87f69d143b9accc86ca86563927","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 990\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='248' max='124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [124/124 15:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running training *****\n","  Num examples = 7389\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 924\n","  Number of trainable parameters = 278047490\n"]},{"name":"stdout","output_type":"stream","text":["Perplexity before training: 2.01\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='924' max='924' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [924/924 14:31, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.548600</td>\n","      <td>0.473495</td>\n","      <td>0.819481</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to test-clm/checkpoint-500\n","Configuration saved in test-clm/checkpoint-500/config.json\n","Model weights saved in test-clm/checkpoint-500/pytorch_model.bin\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 990\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 990\n","  Batch size = 8\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='124' max='124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [124/124 00:31]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Perplexity after training: 1.61\n"]},{"data":{"text/plain":["XLMRobertaForSequenceClassification(\n","  (roberta): XLMRobertaModel(\n","    (embeddings): XLMRobertaEmbeddings(\n","      (word_embeddings): Embedding(250005, 768)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): XLMRobertaEncoder(\n","      (layer): ModuleList(\n","        (0): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): XLMRobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model_gpt2_train(df_lan_dict, lan='english', n_epochs=1)"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":436,"status":"ok","timestamp":1669639224159,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"hKvVgPTyizf5"},"outputs":[],"source":["model_gpt2_train(df_lan_dict, lan='korean', n_epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"ay6xFqj-OFrQ"},"source":["### Evaluation of fine-tunning classification model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We start of with definning some helper functions for evaluating our model. "]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":345,"status":"ok","timestamp":1669645708599,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"n-4mWLHvtWMu"},"outputs":[],"source":["def accuracy(target, pred):\n","    return metrics.accuracy_score(target, pred)\n","\n","def F1(target, pred):\n","    return metrics.f1_score(target, pred)\n","\n","def make_target(label):\n","    return torch.tensor(label, dtype=torch.long)\n","    \n","def evaluate(model: nn.Module, df_dict: dict, lan, loss_function = nn.CrossEntropyLoss()):\n","  \"\"\"\n","  Evaluates the model on the given dataset\n","  :param model: The model under evaluation\n","  :param valid_dl: A `DataLoader` reading validation data\n","  :return: The accuracy of the model on the dataset\n","  \"\"\"\n","  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like \n","  # layer normalization and dropout\n","  model.eval()\n","  logprob_all = []\n","  losses_all = []\n","  target_val = []\n","  predictions = []\n","\n","  with torch.no_grad():\n","      for index, row in df_dict[lan]['val'].iterrows():\n","          instance_context = row['text']\n","          encoded = tokenizer(instance_context,\n","                      return_tensors='pt', truncation = True, padding=True, max_length = 512)\n","          encoded = {k:v.to(device) for k, v in encoded.items()}\n","          model_output = model(**encoded, output_hidden_states=True, return_dict=True)\n","\n","          label = row['label']\n","          target = make_target(label)\n","          target_val.append(target.tolist())\n","\n","          log_probs = model_output.logits[0] ## input CR\n","          #print([log_probs.softmax(dim=-1).detach().cpu().flatten().numpy()[0]])\n","          logprob_all.extend([log_probs.softmax(dim=-1).detach().cpu().flatten().numpy()[0]])\n","\n","          #print(log_probs)\n","\n","  return logprob_all"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The evaluate function will compute the prediction of our models. By setting a threshold of 0.5 for the probabilities we will be able to compute the accuracy of our model based on the validation set. "]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23148,"status":"ok","timestamp":1669645808749,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"dnsSoaJmPSYD","outputId":"4b00cb41-54b7-41a9-b319-af19c1ba859a"},"outputs":[{"data":{"text/plain":["0.8222222222222222"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["no_answer_prob_en = evaluate(model_class, df_lan_dict, 'english', loss_function = nn.CrossEntropyLoss())\n","accuracy(np.array(no_answer_prob_en)<0.5,df_lan_dict['english']['df_val']['label'])"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15780,"status":"ok","timestamp":1669639266008,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"aeTg1YnNug0z","outputId":"86c261ca-89b7-4735-fe64-cf948cd948d8"},"outputs":[{"data":{"text/plain":["0.8654781199351702"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["no_answer_prob_ko = evaluate(model_class, df_lan_dict, 'korean', loss_function = nn.CrossEntropyLoss())\n","accuracy(np.array(no_answer_prob_ko)<0.5,df_lan_dict['korean']['df_val']['label'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Extraction model"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WulIyq1mnUtk"},"source":["### Fine-tuning extraction model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For the extraction model the fine-tuning code and method comes from [Hugging face's](https://huggingface.co/course/chapter7/7?fw=pt) course chapter 7. "]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":410,"status":"ok","timestamp":1669646295738,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"8FK1wl67naf5"},"outputs":[],"source":["def get_train_features(tk, samples):\n","  '''\n","  Tokenizes all of the text in the given samples, splittling inputs that are too long for our model\n","  across multiple features. Finds the token offsets of the answers, which serve as the labels for\n","  our inputs.\n","  '''\n","  batch = tk.batch_encode_plus(\n","        [[q,c] for q,c in zip(samples['question'], samples['context'])], \n","        padding='max_length', \n","        truncation='only_second',\n","        stride=128,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True\n","    )\n","\n","  # Get a list which maps the input features index to their original index in the \n","  # samples list (for split inputs). E.g. if our batch size is 4 and the second sample\n","  # is split into 3 inputs because it is very large, sample_mapping would look like\n","  # [0, 1, 1, 1, 2, 3]\n","  sample_mapping = batch.pop('overflow_to_sample_mapping')\n","  # Get all of the character offsets for each token\n","  offset_mapping = batch.pop('offset_mapping')\n","\n","  # Store the start and end tokens\n","  batch['start_tokens'] = []\n","  batch['end_tokens'] = []\n","\n","  # Iterate through all of the offsets\n","  for i, offsets in enumerate(offset_mapping):\n","    # Get the right sample by mapping it to its original index\n","    sample_idx = sample_mapping[i]\n","    # Get the sequence IDs to know where context starts so we can ignore question tokens\n","    sequence_ids = batch.sequence_ids(i)\n","\n","    # Get the start and end character positions of the answer\n","    ans = samples['answers'][sample_idx]\n","    start_char = ans['answer_start'][0]\n","    end_char = start_char + len(ans['text'][0])\n","    # while end_char > 0 and (end_char >= len(samples['context'][sample_idx]) or samples['context'][sample_idx][end_char] == ' '):\n","    #   end_char -= 1\n","\n","    # Start from the first token in the context, which can be found by going to the \n","    # first token where sequence_ids is 1\n","    start_token = 0\n","    while sequence_ids[start_token] != 1:\n","      start_token += 1\n","\n","    end_token = len(offsets) - 1\n","    while sequence_ids[end_token] != 1:\n","      end_token -= 1\n","\n","    # By default set it to the CLS token if the answer isn't in this input\n","    if start_char < offsets[start_token][0] or end_char > offsets[end_token][1]:\n","      start_token = 0\n","      end_token = 0\n","    # Otherwise find the correct token indices\n","    else:\n","      # Advance the start token index until we have passed the start character index \n","      while start_token < len(offsets) and offsets[start_token][0] <= start_char:\n","        start_token += 1\n","      start_token -= 1\n","      \n","      # Decrease the end token index until we have passed the end character index\n","      while end_token >= 0 and offsets[end_token][1] >= end_char:\n","        end_token -= 1\n","      end_token += 1\n","\n","    batch['start_tokens'].append(start_token)\n","    batch['end_tokens'].append(end_token)\n","\n","  #batch['start_tokens'] = np.array(batch['start_tokens'])\n","  #batch['end_tokens'] = np.array(batch['end_tokens'])\n","\n","  return batch\n","\n","def collate_fn(inputs):\n","  '''\n","  Defines how to combine different samples in a batch\n","  '''\n","  input_ids = torch.tensor([i['input_ids'] for i in inputs])\n","  attention_mask = torch.tensor([i['attention_mask'] for i in inputs])\n","  start_tokens = torch.tensor([i['start_tokens'] for i in inputs])\n","  end_tokens = torch.tensor([i['end_tokens'] for i in inputs])\n","\n","  # Truncate to max length\n","  max_len = max(attention_mask.sum(-1))\n","  input_ids = input_ids[:,:max_len]\n","  attention_mask = attention_mask[:,:max_len]\n","  \n","  return {'input_ids': input_ids, 'attention_mask': attention_mask, 'start_tokens': start_tokens, 'end_tokens': end_tokens}"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669646296196,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"o-0_vMkon5JP"},"outputs":[],"source":["def train(\n","    model: nn.Module, \n","    train_dl: DataLoader, \n","    optimizer: torch.optim.Optimizer, \n","    scheduler: LambdaLR,\n","    n_epochs: int, \n","    device: torch.device,\n","    lan: str\n","):\n","  \"\"\"\n","  The main training loop which will optimize a given model on a given dataset\n","  :param model: The model being optimized\n","  :param train_dl: The training dataset\n","  :param optimizer: The optimizer used to update the model parameters\n","  :param n_epochs: Number of epochs to train for\n","  :param device: The device to train on\n","  \"\"\"\n","\n","  # Keep track of the loss and best accuracy\n","  losses = []\n","\n","  # Iterate through epochs\n","  for ep in range(n_epochs):\n","\n","    loss_epoch = []\n","\n","    #Iterate through each batch in the dataloader\n","    for batch in tqdm(train_dl):\n","      # VERY IMPORTANT: Make sure the model is in training mode, which turns on \n","      # things like dropout and layer normalization\n","      model.train()\n","\n","      # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n","      # keeps track of these dynamically in its computation graph so you need to explicitly\n","      # zero them out\n","      optimizer.zero_grad()\n","\n","      # Place each tensor on the GPU\n","      batch = {b: batch[b].to(device) for b in batch}\n","\n","      # Pass the inputs through the model, get the current loss and logits\n","      outputs = model(\n","          input_ids=batch['input_ids'],\n","          attention_mask=batch['attention_mask'],\n","          start_positions=batch['start_tokens'],\n","          end_positions=batch['end_tokens']\n","      )\n","      loss = outputs['loss']\n","      losses.append(loss.item())\n","      loss_epoch.append(loss.item())\n","      \n","      # Calculate all of the gradients and weight updates for the model\n","      loss.backward()\n","      # Optional: clip gradients\n","      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","      # Finally, update the weights of the model and advance the LR schedule\n","      optimizer.step()\n","      scheduler.step()\n","      #gc.collect()\n","    torch.save(model,'/content/drive/MyDrive/Deep Learning - NLP project/model_extract_m_roberta_' + lan[0:2] + '.pt')\n","    #torch.save(model,'/content/drive/MyDrive/Deep Learning - NLP project/model_RoBERTa_extract_' + lan[0:2] + '.pt')\n","  return losses"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For the following sections of code, the english models are selected, but if the korean models are selected the comment code needs to be run instead. "]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11649,"status":"ok","timestamp":1669646309745,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"R3wAHvhLoDx7","outputId":"1ada82df-3095-4e96-cdf0-0ed5a40db853"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"xlm-roberta-base\",\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.24.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin\n","Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["Embedding(250005, 768)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["model_name_en = all_models[model_name_class]\n","model_en = AutoModelForQuestionAnswering.from_pretrained(model_name_en).to(device)\n","model_en.resize_token_embeddings(len(tokenizer))\n","\n","# If it is the korean model selected\n","#model_ko = AutoModelForQuestionAnswering.from_pretrained(all_models[model_name_class]).to(device)\n","#model_ko.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["82bcbabb011c4adf9e15122c10f13e5e","be700eef45814767b19c95e30aefd57a","6d4d00c3bf4c4e8dbe5a34d3bdc96dab","d65cf8f8c40e4689890033f7dfe8dafb","00338d7eb4e540798e6b583edfdb7c7a","07829b9e80444261821bd1705313e403","f241d0b5c3cf4d2891843a34a5d3158d","e6e8e9cf09e34880acfaffd61c847faa","2d0c280bcd0d40519a9735cda0c0342b","654ce0ab6b664ada9e1402f358dba6a6","f6f3f991d60744a5b5ea065ba9f7a1cd"]},"executionInfo":{"elapsed":2227,"status":"ok","timestamp":1669646311958,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"0dioiNTboGBw","outputId":"fa707f54-a3d2-46cb-9008-4a454be9b701"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82bcbabb011c4adf9e15122c10f13e5e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_dataset_en = en_dataset_squad['train'].map(partial(get_train_features, tokenizer), batched=True, remove_columns=en_dataset_squad['train'].column_names)\n","train_dl_en = DataLoader(tokenized_dataset_en, collate_fn=collate_fn, shuffle=True, batch_size=4)\n","\n","# tokenized_dataset_ko = ko_dataset_squad['train'].map(partial(get_train_features, tokenizer), batched=True, remove_columns=ko_dataset_squad['train'].column_names)\n","# train_dl_ko = DataLoader(tokenized_dataset_ko, collate_fn=collate_fn, shuffle=True, batch_size=4)"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":302,"status":"ok","timestamp":1669646312255,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"ZDeaMDEcodHI"},"outputs":[],"source":["# Create the optimizer\n","lr=3e-5\n","n_epochs = 5\n","weight_decay = 0.01\n","warmup_steps = 250\n","\n","# optimizer = Adam(optimizer_grouped_parameters, lr=1e-3)\n","# scheduler = None\n","optimizer_en = AdamW(model_en.parameters(), lr=lr, betas=(0.9,0.98), eps=1e-6, weight_decay=weight_decay)\n","scheduler_en = get_linear_schedule_with_warmup(\n","    optimizer_en,\n","    warmup_steps,\n","    n_epochs * len(train_dl_en)\n",")\n","\n","# optimizer_ko = AdamW(model_ko.parameters(), lr=lr, betas=(0.9,0.98), eps=1e-6, weight_decay=weight_decay)\n","# scheduler_ko = get_linear_schedule_with_warmup(\n","#     optimizer_ko,\n","#     warmup_steps,\n","#     n_epochs * len(train_dl_ko)\n","# )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can now fine-tune our model"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["c2f179a1c5ab49039ecf3bde55057ac8","068de52e5de84f1dad70555c6a89a272","a20331497d3b46ab92908025c772d263","aaeb7233bc3244aab206583294d8f6cb","03e2ed5b2bdd48a380fa400da820d8d7","806ab0592c9a49de9ebfb25ae3713697","7d3c9fd5fdea48aca4e4f36a32c76f3b","5dd01cca925943faa13179c5d9c11888","d0ffd6ad49114500949ecc6bdd83b06c","a82f1fa7bd1145b6adab3a6126a2a912","e5d7a615a8be48b88afcdf713dd1592e","c782aa7761b74aa88bbcdd690d260d1b","540da5fcba604285aa6e69ed2070220c","65ecca752100462a876fe91e68f92439","566be494ccd84fe2b9d443101a5843f7","0061885fa9394d8b84f9096d357f6ecd","e8eb2deec7024113b79c9ad0150ff77f","65d6cc705e0945ee81de55601fbfcd0f","b2a64d005b324fb48c693fe37123fe45","e0e00903926749dba8387969875bdba3","db32d2d2c7f34c92bf6c8b60b9f70412","517150ccb7084003a7184f43ef074f4c","7d7b017828c24b99a16eefcb441d8d32","1f65e81625074fa1b9bf22820e41041a","6991e138484f4df7a38eb12d7bb95f8e","7760363ff274431fb0d6d63083f76f81","4f1fa96087f44a988b7e3b04bb617a28","ea83cf481eb14048b7ae59487758a9b2","a9f3517d976d49ffbd60aa615757fc29","effcad38fa6b4d63acaa3e9d4d99e09c","9ce7a2ecc7e14dd59f06f06d16dbf144","7213a464f0654afebc47e344bd760777","fd745b48adf24f479111281dc891df45","d614f45c4b07408ea3e4c04ba5a3a709","e8a1770da82c492b89dedc801c3dc475","c5c5b483fd0a4be2802e859345c979c9","e6260ec8b3204b8a89bbf09b9ea6f288","f8090f13f27c43e4bb2412c5434314a7","11ebaf4b71ed466b81c0e1eb4c5c1268","6fe333d860394a97b1ecbdf4e413cf46","4b9c27fac7c94966977589dce443cb73","44bcc618a8a449f595024a1a7d420f9e","2f0d33afd5d5476a8be8db0d633874bd","de796767110944cfa1393604bfe38234","b88c84deda1240218201e24412a53e80","08935dc52e884384abf62361e424984a","a5375d210bc541fc8f7b95dd0fab37a1","877deb59599e4927993ff6c7b6522e1a","70e1673207d44fe59531ce4e0e4f3933","60e453b7149b4775af643913239a73dd","324def1fb94f41a5aa0d961b6b5b45c9","0b1f11cefca443d49d7a4aadf6845020","d208262e620a4c6680f871b980872a3a","9b24805ff0224b2a8da9760974c17368","d1c4773413514275b365c0dea55d2dac"]},"executionInfo":{"elapsed":1633535,"status":"ok","timestamp":1669647945782,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"ktWbh6ckog_4","outputId":"53b1521e-e284-43e8-8729-26da4c22c77f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2f179a1c5ab49039ecf3bde55057ac8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/938 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c782aa7761b74aa88bbcdd690d260d1b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/938 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d7b017828c24b99a16eefcb441d8d32","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/938 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d614f45c4b07408ea3e4c04ba5a3a709","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/938 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b88c84deda1240218201e24412a53e80","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/938 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["losses = train(\n","    model_en, \n","    train_dl_en,\n","    optimizer_en, \n","    scheduler_en,\n","    n_epochs, \n","    device,\n","    'english'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAI79qAfojmB"},"outputs":[],"source":["# Training for Korean extraction model\n","#  losses = train(\n","#     model_ko, \n","#     train_dl_ko,\n","#     optimizer_ko, \n","#     scheduler_ko,\n","#     n_epochs, \n","#     device,\n","#     'korean'\n","# )"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2hf3mjUMQp2t"},"source":["### Evaluate fine-tuned extraction model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before we can evaluate the models we will define some helper functions. "]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1669647945787,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"R_YeVRNFomov"},"outputs":[],"source":["def get_validation_features(tk, samples):\n","  # First, tokenize the text. We get the offsets and return overflowing sequences in\n","  # order to break up long sequences into multiple inputs. The offsets will help us \n","  # determine the original answer text \n","  batch = tk.batch_encode_plus(\n","        [[q,c] for q,c in zip(samples['question'], samples['context'])], \n","        padding='max_length', \n","        truncation='only_second',\n","        stride=128,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True\n","    )\n","  \n","  # We'll store the ID of the samples to calculate squad score\n","  batch['example_id'] = []\n","  # The overflow sample map tells us which input each sample corresponds to\n","  sample_map = batch.pop('overflow_to_sample_mapping')\n","\n","  for i in range(len(batch['input_ids'])):\n","    # The sample index tells us which of the values in \"samples\" these features belong to\n","    sample_idx = sample_map[i]\n","    sequence_ids = batch.sequence_ids(i)\n","\n","    # Add the ID to map these features back to the correct sample\n","    batch['example_id'].append(samples['id'][sample_idx])\n","\n","    #Set offsets for non-context words to be None for ease of processing\n","    batch['offset_mapping'][i] = [o if sequence_ids[k] == 1 else None for k,o in enumerate(batch['offset_mapping'][i])]\n","\n","  return batch  \n","\n","def val_collate_fn(inputs):\n","  input_ids = torch.tensor([i['input_ids'] for i in inputs])\n","  attention_mask = torch.tensor([i['attention_mask'] for i in inputs])\n","\n","  # Truncate to max length\n","  max_len = max(attention_mask.sum(-1))\n","  input_ids = input_ids[:,:max_len]\n","  attention_mask = attention_mask[:,:max_len]\n","  \n","  return {'input_ids': input_ids, 'attention_mask': attention_mask}"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["2425cf52e4a443a6921b2e7a8ceec9c0","79cb50e44d4045849bc5496bafe5b69d","01dd53ef881741a2a8385691afbd034c","85312fca254f468698756671e806a6d0","acd1f83113d642c78a238171834d5ade","ce7d56e7c19e4be4ae067e1ba3f065ca","e0a3421b9f7e41a1a71b411b4966aaa3","b8aedf41a17f440888e1d3161be3c207","f76a3903e9794057adccea363369c42e","bd441be793b245b485a7db76aee7c059","7326517d35854a318f42e09c64fbbf7e","e3ba11de39d14aa7822aeffb03fb151e","152a5b89534b4e5c970c37d241f1b430","2555ec332edb442188cdc1ca9dcc483d","96f16f4bea9647d6ac5bf94a9b28902b","7d9e2d17c0ff4defac90702c7c60a2e8","acad7767d02143ed91c2bb8f9070d3bd","12b91581279f40a290a65c26910f9968","48d956e5fd0e4f50a4d18957271648d3","895bcc80178e4913946cfd34237696c1","fac18486d7aa431098228366e845382e","faaae34486714069a482d8b7a3dca0c3"]},"executionInfo":{"elapsed":1167,"status":"ok","timestamp":1669647946932,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"0k-18mKtoxcG","outputId":"0c66d512-407e-4d92-eddc-ccdc889dabd0"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2425cf52e4a443a6921b2e7a8ceec9c0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3ba11de39d14aa7822aeffb03fb151e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["validation_dataset_en = en_dataset_squad['validation_full'].map(partial(get_validation_features, tokenizer), batched=True, remove_columns=en_dataset_squad['validation_full'].column_names)\n","validation_dataset_ko = ko_dataset_squad['validation_full'].map(partial(get_validation_features, tokenizer), batched=True, remove_columns=ko_dataset_squad['validation_full'].column_names)"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1669647946933,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"usm7OemNo0NL"},"outputs":[],"source":["def predict(model: nn.Module, valid_dl: DataLoader):\n","  \"\"\"\n","  Evaluates the model on the given dataset\n","  :param model: The model under evaluation\n","  :param valid_dl: A `DataLoader` reading validation data\n","  :return: The accuracy of the model on the dataset\n","  \"\"\"\n","  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like \n","  # layer normalization and dropout\n","  model.eval()\n","  start_logits_all = []\n","  end_logits_all = []\n","\n","  # ALSO IMPORTANT: Don't accumulate gradients during this process\n","  with torch.no_grad():\n","    for batch in tqdm(valid_dl, desc='Evaluation'):\n","      batch = {b: batch[b].to(device) for b in batch}\n","\n","      # Pass the inputs through the model, get the current loss and logits\n","      outputs = model(\n","          input_ids=batch['input_ids'],\n","          attention_mask=batch['attention_mask']\n","      )\n","      # Store the \"start\" class logits and \"end\" class logits for every token in the input\n","      start_logits_all.extend(list(outputs['start_logits'].detach().cpu().numpy()))\n","      end_logits_all.extend(list(outputs['end_logits'].detach().cpu().numpy()))\n","\n","\n","    return start_logits_all,end_logits_all\n","\n","def post_process_predictions(examples, dataset, logits, num_possible_answers = 20, max_answer_length = 30):\n","  all_start_logits, all_end_logits = logits\n","  # Build a map from example to its corresponding features. This will allow us to index from\n","  # sample ID to all of the features for that sample (in case they were split up due to long input)\n","  example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n","  features_per_example = defaultdict(list)\n","  for i, feature in enumerate(dataset):\n","      features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n","\n","  # Create somewhere to store our predictions\n","  predictions = OrderedDict()\n","\n","  # Iterate through each sample in the dataset\n","  for j, sample in enumerate(tqdm(examples)):\n","\n","    # Get the feature indices (all of the features split across the batch)\n","    feature_indices = features_per_example[j]\n","    # Get the original context which predumably has the answer text\n","    context = sample['context']\n","\n","    preds = []\n","    # Iterate through all of the features\n","    for ft_idx in feature_indices:\n","\n","      # Get the start and end answer logits for this input\n","      start_logits = all_start_logits[ft_idx]\n","      end_logits = all_end_logits[ft_idx]\n","\n","      # Get the offsets to map token indices to character indices\n","      offset_mapping = dataset[ft_idx]['offset_mapping']\n","\n","      # Sort the logits and take the top N\n","      start_indices = np.argsort(start_logits)[::-1][:num_possible_answers]\n","      end_indices = np.argsort(end_logits)[::-1][:num_possible_answers]\n","\n","      # Iterate through start and end indices\n","      for start_index in start_indices:\n","        for end_index in end_indices:\n","          \n","          # Ignore this combination if either the indices are not in the context\n","          if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or offset_mapping[end_index] is None:\n","            continue\n","\n","          # Also ignore if the start index is greater than the end index of the number of tokens\n","          # is greater than some specified threshold\n","          if start_index > end_index or end_index - start_index + 1 > max_answer_length:\n","            continue\n","\n","          ans_text = context[offset_mapping[start_index][0]:offset_mapping[end_index][1]]\n","          preds.append({\n","              'score': start_logits[start_index] + end_logits[end_index],\n","              'text': ans_text\n","          })\n","\n","    if len(preds) > 0:\n","      # Sort by score to get the top answer\n","      answer = sorted(preds, key=lambda x: x['score'], reverse=True)[0]\n","    else:\n","      answer = {'score': 0.0, 'text': \"\"}\n","          \n","    predictions[sample['id']] = answer['text']\n","  return predictions"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":380,"status":"ok","timestamp":1669647947307,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"iKSpjJzKo8Uv"},"outputs":[],"source":["compute_squadv2 = load_metric(\"squad_v2\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before we move on, we found a small problem in hugging face's metric squad_v2, thus a small change was implemented by us."]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":464,"status":"ok","timestamp":1669647947769,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"qoy8MfiSS0JI"},"outputs":[],"source":["# Evaluation metric\n","from datasets import load_metric\n","import collections\n","compute_squad = load_metric(\"squad\")\n","\n","## Official evaluation functions from huggingface, there was a small error which we fixed such it showed for with Ans and NoAns\n","# FROM https://github.com/huggingface/datasets/blob/df4bdd365f2abb695f113cbf8856a925bc70901b/metrics/squad_v2/evaluate.py#L105\n","\n","def normalize_answer(s):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    def remove_articles(text):\n","        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","\n","def get_tokens(s):\n","    if not s:\n","        return []\n","    return normalize_answer(s).split()\n","\n","\n","def make_qid_to_has_ans(dataset):\n","    qid_to_has_ans = {}\n","    for article in dataset:\n","        for p in article[\"paragraphs\"]:\n","            for qa in p[\"qas\"]:\n","                qid_to_has_ans[qa[\"id\"]] = bool(qa[\"answers\"][\"text\"])\n","    return qid_to_has_ans\n","\n","def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n","    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n","    cur_score = num_no_ans\n","    best_score = cur_score\n","    best_thresh = 0.0\n","    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n","    for i, qid in enumerate(qid_list):\n","        if qid not in scores:\n","            continue\n","        if qid_to_has_ans[qid]:\n","            diff = scores[qid]\n","        else:\n","            if preds[qid]:\n","                diff = -1\n","            else:\n","                diff = 0\n","        cur_score += diff\n","        if cur_score > best_score:\n","            best_score = cur_score\n","            best_thresh = na_probs[qid]\n","    return 100.0 * best_score / len(scores), best_thresh\n","\n","\n","def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n","    new_scores = {}\n","    for qid, s in scores.items():\n","        pred_na = na_probs[qid] > na_prob_thresh\n","        if pred_na:\n","            new_scores[qid] = float(not qid_to_has_ans[qid])\n","        else:\n","            new_scores[qid] = s\n","    return new_scores\n","\n","def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n","    if not qid_list:\n","        total = len(exact_scores)\n","        return collections.OrderedDict(\n","            [\n","                (\"exact\", 100.0 * sum(exact_scores.values()) / total),\n","                (\"f1\", 100.0 * sum(f1_scores.values()) / total),\n","                (\"total\", total),\n","            ]\n","        )\n","    else:\n","        total = len(qid_list)\n","        return collections.OrderedDict(\n","            [\n","                (\"exact\", 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n","                (\"f1\", 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n","                (\"total\", total),\n","            ]\n","        )\n","\n","def merge_eval(main_eval, new_eval, prefix):\n","    for k in new_eval:\n","        main_eval[f\"{prefix}_{k}\"] = new_eval[k]\n","\n","def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n","    best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n","    best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n","    main_eval[\"best_exact\"] = best_exact\n","    main_eval[\"best_exact_thresh\"] = exact_thresh\n","    main_eval[\"best_f1\"] = best_f1\n","    main_eval[\"best_f1_thresh\"] = f1_thresh\n","\n","\n","def compute_f1(a_gold, a_pred):\n","    gold_toks = get_tokens(a_gold)\n","    pred_toks = get_tokens(a_pred)\n","    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","    num_same = sum(common.values())\n","    if len(gold_toks) == 0 or len(pred_toks) == 0:\n","        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n","        return int(gold_toks == pred_toks)\n","    if num_same == 0:\n","        return 0\n","    precision = 1.0 * num_same / len(pred_toks)\n","    recall = 1.0 * num_same / len(gold_toks)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","\n","def compute_exact(a_gold, a_pred):\n","    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n","\n","\n","def get_raw_scores(dataset, preds):\n","    exact_scores = {}\n","    f1_scores = {}\n","    for article in dataset:\n","        for p in article[\"paragraphs\"]:\n","            for qa in p[\"qas\"]:\n","                qid = qa[\"id\"]\n","                gold_answers = [qa[\"answers\"][\"text\"]] # Changes made by us\n","\n","                if not gold_answers:\n","                    # For unanswerable questions, only correct answer is empty string\n","                    gold_answers = [\"\"]\n","                if qid not in preds:\n","                    print(f\"Missing prediction for {qid}\")\n","                    continue\n","                a_pred = preds[qid]\n","                # Take max over all gold answers\n","                exact_scores[qid] =  max(compute_exact(a[0], a_pred) for a in gold_answers) \n","                f1_scores[qid] = max(compute_f1(a[0], a_pred) for a in gold_answers)\n","    return exact_scores, f1_scores\n","\n","# FROM https://github.com/huggingface/datasets/blob/main/metrics/squad_v2/squad_v2.py\n","def compute_ans_unans(predictions, references, no_answer_threshold=1.0):\n","    no_answer_probabilities = {p[\"id\"]: p[\"no_answer_probability\"] for p in predictions}\n","    dataset = [{\"paragraphs\": [{\"qas\": references}]}]\n","    predictions = {p[\"id\"]: p[\"prediction_text\"] for p in predictions}\n","\n","    qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n","    has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n","    no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n","\n","    exact_raw, f1_raw = get_raw_scores(dataset, predictions)\n","    exact_thresh = apply_no_ans_threshold(exact_raw, no_answer_probabilities, qid_to_has_ans, no_answer_threshold)\n","    f1_thresh = apply_no_ans_threshold(f1_raw, no_answer_probabilities, qid_to_has_ans, no_answer_threshold)\n","    out_eval = make_eval_dict(exact_thresh, f1_thresh)\n","\n","    if has_ans_qids:\n","        has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n","        merge_eval(out_eval, has_ans_eval, \"HasAns\")\n","    if no_ans_qids:\n","        no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n","        merge_eval(out_eval, no_ans_eval, \"NoAns\")\n","    find_all_best_thresh(out_eval, predictions, exact_raw, f1_raw, no_answer_probabilities, qid_to_has_ans)\n","    return dict(out_eval)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We will predict the answers to the question using our fine-tuned model on the validation set. Thereafter we would also add the classification model to produce the full result. "]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":13,"status":"error","timestamp":1669647947770,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"DXPiyTcrpFTT","outputId":"22ef6902-20e3-4998-da31-9b9ff68537c7"},"outputs":[],"source":["val_dl_en = DataLoader(validation_dataset_en, collate_fn=val_collate_fn, batch_size=32)\n","logits_en = predict(model_extract, val_dl_en)\n","predictions_en = post_process_predictions(en_dataset_squad['validation_full'], validation_dataset_en, logits_en)\n","formatted_predictions_en = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0} for k,v in predictions_en.items()]\n","for i in range(len(no_answer_prob_en)):\n","  formatted_predictions_en[i]['no_answer_probability'] = no_answer_prob_en[i]\n","  if no_answer_prob_en[i] > 0.5:\n","    formatted_predictions_en[i]['prediction_text'] = ''\n","gold_en = [{'id': example['id'], 'answers': example['answers']} for example in en_dataset_squad['validation_full']]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1669647947771,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"NUbVMvaaYBRS"},"outputs":[],"source":["compute_ans_unans(predictions = formatted_predictions_en, references = gold_en)"]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["f6d1c1f4408847ee83b076b215506964","d55d466ce22d4bff96085792479b95b8","9b2f5abaa38c4d19981c806614de6b13","dd66f1c489b446c0bf6ed4ba002d4b0a","0d9df55f91944327aee6b42421910096","a6fdeb810472435dbb0f36ea5679604f","768fc8f0e6fa4c86b477c049345f986b","4681b98181844740932ffab198e06cb5","96c464dfb10e4e6ba121b6926ccc2773","b2110f8f12ff4254af326b62ec96842f","8e86fc41c675488a94d4f721d9457af0","5216bdb59a684b47884e7bf6cf490cc9","4145545578ba41528c3717e2537bf303","7af8651121ad4e1ebecd4bc3c05e8fb1","6cffaf8a78c8408db5999501b23ff25d","ffc7cccd507242d4844c4a7f59e61152","5849a36198694c07ba7f5239c11ef2be","1ead7193efe3409d82dd125491d354fe","b4ee3399602945b5a5bcdedab9290a7a","e52adff3331b4ee3896ae644b7e8cc99","a7a1aab3601d4c23b6cd3ece952c1514","2529f16f962b41a4be8928fb56c1dcfa"]},"executionInfo":{"elapsed":21864,"status":"ok","timestamp":1669641129990,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"rHuKvpwMTdhL","outputId":"49ba2fe4-d566-4642-d0eb-65ceb71fabae"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6d1c1f4408847ee83b076b215506964","version_major":2,"version_minor":0},"text/plain":["Evaluation:   0%|          | 0/20 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5216bdb59a684b47884e7bf6cf490cc9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/617 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["val_dl_ko = DataLoader(validation_dataset_ko, collate_fn=val_collate_fn, batch_size=32)\n","logits_ko = predict(model_extract, val_dl_ko)\n","predictions_ko = post_process_predictions(ko_dataset_squad['validation_full'], validation_dataset_ko, logits_ko)\n","formatted_predictions_ko = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0} for k,v in predictions_ko.items()]\n","for i in range(len(no_answer_prob_ko)):\n","  formatted_predictions_ko[i]['no_answer_probability'] = no_answer_prob_ko[i]\n","  if no_answer_prob_ko[i] > 0.5:\n","    formatted_predictions_ko[i]['prediction_text'] = ''\n","gold_ko = [{'id': example['id'], 'answers': example['answers']} for example in ko_dataset_squad['validation_full']]"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1669641130227,"user":{"displayName":"Mia Hang Knudsen","userId":"09534110648088576151"},"user_tz":-60},"id":"uJIFclEPTF_H","outputId":"6cc78cce-eede-4696-a382-f4820f486aa9"},"outputs":[{"data":{"text/plain":["{'exact': 66.12641815235008,\n"," 'f1': 69.59001858558287,\n"," 'total': 617,\n"," 'HasAns_exact': 66.12641815235008,\n"," 'HasAns_f1': 69.59001858558287,\n"," 'HasAns_total': 617,\n"," 'best_exact': 66.12641815235008,\n"," 'best_exact_thresh': 0.9832002,\n"," 'best_f1': 69.59001858558287,\n"," 'best_f1_thresh': 0.9832002}"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["compute_ans_unans(predictions = formatted_predictions_ko, references = gold_ko)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO9TY+Cj8lx5M2od09/QAGg","collapsed_sections":["ay6xFqj-OFrQ","WulIyq1mnUtk","KSPJXYE4nXKT"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"vscode":{"interpreter":{"hash":"5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"00338d7eb4e540798e6b583edfdb7c7a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0061885fa9394d8b84f9096d357f6ecd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01dd53ef881741a2a8385691afbd034c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8aedf41a17f440888e1d3161be3c207","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f76a3903e9794057adccea363369c42e","value":1}},"02bf58e376f5429a9af749cc5853ee24":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03e2ed5b2bdd48a380fa400da820d8d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05c6f06505e54e829c5afc18f07af47d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"068de52e5de84f1dad70555c6a89a272":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_806ab0592c9a49de9ebfb25ae3713697","placeholder":"","style":"IPY_MODEL_7d3c9fd5fdea48aca4e4f36a32c76f3b","value":"100%"}},"07829b9e80444261821bd1705313e403":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08935dc52e884384abf62361e424984a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60e453b7149b4775af643913239a73dd","placeholder":"","style":"IPY_MODEL_324def1fb94f41a5aa0d961b6b5b45c9","value":"100%"}},"09009fc3c3c344c2b08e2df9b0e87e1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd36d143a46d4c0dae8f46ac7b97fa3a","max":1115590446,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89e1a06a857a4dcbbdadbdabc529a0a4","value":1115590446}},"0b1f11cefca443d49d7a4aadf6845020":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d9df55f91944327aee6b42421910096":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11ebaf4b71ed466b81c0e1eb4c5c1268":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12b91581279f40a290a65c26910f9968":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1334c3ecc63c42a9bf779fb673f965d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"152a5b89534b4e5c970c37d241f1b430":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acad7767d02143ed91c2bb8f9070d3bd","placeholder":"","style":"IPY_MODEL_12b91581279f40a290a65c26910f9968","value":"100%"}},"1ead7193efe3409d82dd125491d354fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f65e81625074fa1b9bf22820e41041a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea83cf481eb14048b7ae59487758a9b2","placeholder":"","style":"IPY_MODEL_a9f3517d976d49ffbd60aa615757fc29","value":"100%"}},"2425cf52e4a443a6921b2e7a8ceec9c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_79cb50e44d4045849bc5496bafe5b69d","IPY_MODEL_01dd53ef881741a2a8385691afbd034c","IPY_MODEL_85312fca254f468698756671e806a6d0"],"layout":"IPY_MODEL_acd1f83113d642c78a238171834d5ade"}},"2529f16f962b41a4be8928fb56c1dcfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2555ec332edb442188cdc1ca9dcc483d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_48d956e5fd0e4f50a4d18957271648d3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_895bcc80178e4913946cfd34237696c1","value":1}},"2d0c280bcd0d40519a9735cda0c0342b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ed96f21e38841b88db2d9428406e50e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f92b95b91ef74de68b960625d918bc11","IPY_MODEL_09009fc3c3c344c2b08e2df9b0e87e1c","IPY_MODEL_6c7ea21ddf9f459fabaeb31d62d6cb93"],"layout":"IPY_MODEL_f8f1912d88ed4198b7925dde37f3ad3b"}},"2f0d33afd5d5476a8be8db0d633874bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3016f7fb12ab4822923c0bd90d1e7e44":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"324def1fb94f41a5aa0d961b6b5b45c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35e655cd5fe24be5886af36048da6314":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60c42d13b8c44b3f82c8133ae6d4b281","placeholder":"","style":"IPY_MODEL_93c468a4823946469fe276bb0ab7c29c","value":" 8/8 [00:03&lt;00:00,  2.64ba/s]"}},"40228fafb15b49d193df1df3e68d6c9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4145545578ba41528c3717e2537bf303":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5849a36198694c07ba7f5239c11ef2be","placeholder":"","style":"IPY_MODEL_1ead7193efe3409d82dd125491d354fe","value":"100%"}},"44bcc618a8a449f595024a1a7d420f9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44d1540a40534b309843b31fc401420c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4681b98181844740932ffab198e06cb5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48d956e5fd0e4f50a4d18957271648d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b9c27fac7c94966977589dce443cb73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f1921cbe3fb46c59090c640d4103a0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f1fa96087f44a988b7e3b04bb617a28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"517150ccb7084003a7184f43ef074f4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5216bdb59a684b47884e7bf6cf490cc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4145545578ba41528c3717e2537bf303","IPY_MODEL_7af8651121ad4e1ebecd4bc3c05e8fb1","IPY_MODEL_6cffaf8a78c8408db5999501b23ff25d"],"layout":"IPY_MODEL_ffc7cccd507242d4844c4a7f59e61152"}},"540da5fcba604285aa6e69ed2070220c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8eb2deec7024113b79c9ad0150ff77f","placeholder":"","style":"IPY_MODEL_65d6cc705e0945ee81de55601fbfcd0f","value":"100%"}},"566be494ccd84fe2b9d443101a5843f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db32d2d2c7f34c92bf6c8b60b9f70412","placeholder":"","style":"IPY_MODEL_517150ccb7084003a7184f43ef074f4c","value":" 938/938 [05:18&lt;00:00,  3.22it/s]"}},"580dca12a6a645448981a1bce967cea1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40228fafb15b49d193df1df3e68d6c9a","placeholder":"","style":"IPY_MODEL_8022db8e1d4c4fe190acefe3d0ac54a4","value":" 1/1 [00:00&lt;00:00,  1.99ba/s]"}},"5849a36198694c07ba7f5239c11ef2be":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c153fb69e25484fa78d47ec41df9ca7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dd01cca925943faa13179c5d9c11888":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60c42d13b8c44b3f82c8133ae6d4b281":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60e453b7149b4775af643913239a73dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"654ce0ab6b664ada9e1402f358dba6a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65d6cc705e0945ee81de55601fbfcd0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65ecca752100462a876fe91e68f92439":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2a64d005b324fb48c693fe37123fe45","max":938,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0e00903926749dba8387969875bdba3","value":938}},"6787addb441b420b834d48d087a58204":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02bf58e376f5429a9af749cc5853ee24","placeholder":"","style":"IPY_MODEL_9a99022734fc4112a647037702c458c7","value":"100%"}},"6991e138484f4df7a38eb12d7bb95f8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_effcad38fa6b4d63acaa3e9d4d99e09c","max":938,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ce7a2ecc7e14dd59f06f06d16dbf144","value":938}},"6c7ea21ddf9f459fabaeb31d62d6cb93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f28e4c381bde408c899e876a5d3075c8","placeholder":"","style":"IPY_MODEL_f49912a08ea7433ba389e920afdee97d","value":" 1.12G/1.12G [00:19&lt;00:00, 57.7MB/s]"}},"6cffaf8a78c8408db5999501b23ff25d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7a1aab3601d4c23b6cd3ece952c1514","placeholder":"","style":"IPY_MODEL_2529f16f962b41a4be8928fb56c1dcfa","value":" 617/617 [00:01&lt;00:00, 506.76it/s]"}},"6d4d00c3bf4c4e8dbe5a34d3bdc96dab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6e8e9cf09e34880acfaffd61c847faa","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d0c280bcd0d40519a9735cda0c0342b","value":4}},"6fe333d860394a97b1ecbdf4e413cf46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70e1673207d44fe59531ce4e0e4f3933":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7213a464f0654afebc47e344bd760777":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7326517d35854a318f42e09c64fbbf7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"768fc8f0e6fa4c86b477c049345f986b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7760363ff274431fb0d6d63083f76f81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7213a464f0654afebc47e344bd760777","placeholder":"","style":"IPY_MODEL_fd745b48adf24f479111281dc891df45","value":" 938/938 [05:30&lt;00:00,  3.27it/s]"}},"79cb50e44d4045849bc5496bafe5b69d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce7d56e7c19e4be4ae067e1ba3f065ca","placeholder":"","style":"IPY_MODEL_e0a3421b9f7e41a1a71b411b4966aaa3","value":"100%"}},"7af8651121ad4e1ebecd4bc3c05e8fb1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4ee3399602945b5a5bcdedab9290a7a","max":617,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e52adff3331b4ee3896ae644b7e8cc99","value":617}},"7d3c9fd5fdea48aca4e4f36a32c76f3b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d7b017828c24b99a16eefcb441d8d32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f65e81625074fa1b9bf22820e41041a","IPY_MODEL_6991e138484f4df7a38eb12d7bb95f8e","IPY_MODEL_7760363ff274431fb0d6d63083f76f81"],"layout":"IPY_MODEL_4f1fa96087f44a988b7e3b04bb617a28"}},"7d9e2d17c0ff4defac90702c7c60a2e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8022db8e1d4c4fe190acefe3d0ac54a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"806ab0592c9a49de9ebfb25ae3713697":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82bcbabb011c4adf9e15122c10f13e5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be700eef45814767b19c95e30aefd57a","IPY_MODEL_6d4d00c3bf4c4e8dbe5a34d3bdc96dab","IPY_MODEL_d65cf8f8c40e4689890033f7dfe8dafb"],"layout":"IPY_MODEL_00338d7eb4e540798e6b583edfdb7c7a"}},"85312fca254f468698756671e806a6d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd441be793b245b485a7db76aee7c059","placeholder":"","style":"IPY_MODEL_7326517d35854a318f42e09c64fbbf7e","value":" 1/1 [00:00&lt;00:00,  1.45ba/s]"}},"877deb59599e4927993ff6c7b6522e1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b24805ff0224b2a8da9760974c17368","placeholder":"","style":"IPY_MODEL_d1c4773413514275b365c0dea55d2dac","value":" 938/938 [05:20&lt;00:00,  3.70it/s]"}},"88dad87f69d143b9accc86ca86563927":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da0642f7ca914715b2aced4dd1db3ae4","IPY_MODEL_ccf9e60775e2400abb582801dff0d3cb","IPY_MODEL_580dca12a6a645448981a1bce967cea1"],"layout":"IPY_MODEL_5c153fb69e25484fa78d47ec41df9ca7"}},"895bcc80178e4913946cfd34237696c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89e1a06a857a4dcbbdadbdabc529a0a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e86fc41c675488a94d4f721d9457af0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93c468a4823946469fe276bb0ab7c29c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96c464dfb10e4e6ba121b6926ccc2773":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"96f16f4bea9647d6ac5bf94a9b28902b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fac18486d7aa431098228366e845382e","placeholder":"","style":"IPY_MODEL_faaae34486714069a482d8b7a3dca0c3","value":" 1/1 [00:00&lt;00:00,  2.33ba/s]"}},"9a99022734fc4112a647037702c458c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b24805ff0224b2a8da9760974c17368":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b2f5abaa38c4d19981c806614de6b13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4681b98181844740932ffab198e06cb5","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96c464dfb10e4e6ba121b6926ccc2773","value":20}},"9ce7a2ecc7e14dd59f06f06d16dbf144":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a20331497d3b46ab92908025c772d263":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dd01cca925943faa13179c5d9c11888","max":938,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0ffd6ad49114500949ecc6bdd83b06c","value":938}},"a5375d210bc541fc8f7b95dd0fab37a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b1f11cefca443d49d7a4aadf6845020","max":938,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d208262e620a4c6680f871b980872a3a","value":938}},"a6fdeb810472435dbb0f36ea5679604f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7a1aab3601d4c23b6cd3ece952c1514":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a82f1fa7bd1145b6adab3a6126a2a912":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9f3517d976d49ffbd60aa615757fc29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aaeb7233bc3244aab206583294d8f6cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a82f1fa7bd1145b6adab3a6126a2a912","placeholder":"","style":"IPY_MODEL_e5d7a615a8be48b88afcdf713dd1592e","value":" 938/938 [05:18&lt;00:00,  3.04it/s]"}},"acad7767d02143ed91c2bb8f9070d3bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acd1f83113d642c78a238171834d5ade":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2110f8f12ff4254af326b62ec96842f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2a64d005b324fb48c693fe37123fe45":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4ee3399602945b5a5bcdedab9290a7a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b88c84deda1240218201e24412a53e80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_08935dc52e884384abf62361e424984a","IPY_MODEL_a5375d210bc541fc8f7b95dd0fab37a1","IPY_MODEL_877deb59599e4927993ff6c7b6522e1a"],"layout":"IPY_MODEL_70e1673207d44fe59531ce4e0e4f3933"}},"b8aedf41a17f440888e1d3161be3c207":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc89a2051096408480e72fa0aa5f9fa9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6787addb441b420b834d48d087a58204","IPY_MODEL_d17c2e3ddf0e441885981364424f5734","IPY_MODEL_35e655cd5fe24be5886af36048da6314"],"layout":"IPY_MODEL_be0dc8d109364014997578f7eafe0aa7"}},"bd441be793b245b485a7db76aee7c059":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be0dc8d109364014997578f7eafe0aa7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be700eef45814767b19c95e30aefd57a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07829b9e80444261821bd1705313e403","placeholder":"","style":"IPY_MODEL_f241d0b5c3cf4d2891843a34a5d3158d","value":"100%"}},"c25ec103e1ba4d30bc2066afd3530d78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2f179a1c5ab49039ecf3bde55057ac8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_068de52e5de84f1dad70555c6a89a272","IPY_MODEL_a20331497d3b46ab92908025c772d263","IPY_MODEL_aaeb7233bc3244aab206583294d8f6cb"],"layout":"IPY_MODEL_03e2ed5b2bdd48a380fa400da820d8d7"}},"c3fef546180940688390903fd2a29e86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5c5b483fd0a4be2802e859345c979c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b9c27fac7c94966977589dce443cb73","max":938,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44bcc618a8a449f595024a1a7d420f9e","value":938}},"c782aa7761b74aa88bbcdd690d260d1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_540da5fcba604285aa6e69ed2070220c","IPY_MODEL_65ecca752100462a876fe91e68f92439","IPY_MODEL_566be494ccd84fe2b9d443101a5843f7"],"layout":"IPY_MODEL_0061885fa9394d8b84f9096d357f6ecd"}},"c8576626ae9342a580941ae95e30c3e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccf9e60775e2400abb582801dff0d3cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3016f7fb12ab4822923c0bd90d1e7e44","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f1921cbe3fb46c59090c640d4103a0a","value":1}},"ce7d56e7c19e4be4ae067e1ba3f065ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0ffd6ad49114500949ecc6bdd83b06c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d17c2e3ddf0e441885981364424f5734":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c25ec103e1ba4d30bc2066afd3530d78","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3fef546180940688390903fd2a29e86","value":8}},"d1c4773413514275b365c0dea55d2dac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d208262e620a4c6680f871b980872a3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d55d466ce22d4bff96085792479b95b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6fdeb810472435dbb0f36ea5679604f","placeholder":"","style":"IPY_MODEL_768fc8f0e6fa4c86b477c049345f986b","value":"Evaluation: 100%"}},"d614f45c4b07408ea3e4c04ba5a3a709":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e8a1770da82c492b89dedc801c3dc475","IPY_MODEL_c5c5b483fd0a4be2802e859345c979c9","IPY_MODEL_e6260ec8b3204b8a89bbf09b9ea6f288"],"layout":"IPY_MODEL_f8090f13f27c43e4bb2412c5434314a7"}},"d65cf8f8c40e4689890033f7dfe8dafb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_654ce0ab6b664ada9e1402f358dba6a6","placeholder":"","style":"IPY_MODEL_f6f3f991d60744a5b5ea065ba9f7a1cd","value":" 4/4 [00:02&lt;00:00,  1.57ba/s]"}},"da0642f7ca914715b2aced4dd1db3ae4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44d1540a40534b309843b31fc401420c","placeholder":"","style":"IPY_MODEL_c8576626ae9342a580941ae95e30c3e5","value":"100%"}},"db32d2d2c7f34c92bf6c8b60b9f70412":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd66f1c489b446c0bf6ed4ba002d4b0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2110f8f12ff4254af326b62ec96842f","placeholder":"","style":"IPY_MODEL_8e86fc41c675488a94d4f721d9457af0","value":" 20/20 [00:19&lt;00:00,  1.13it/s]"}},"de796767110944cfa1393604bfe38234":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0a3421b9f7e41a1a71b411b4966aaa3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0e00903926749dba8387969875bdba3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3ba11de39d14aa7822aeffb03fb151e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_152a5b89534b4e5c970c37d241f1b430","IPY_MODEL_2555ec332edb442188cdc1ca9dcc483d","IPY_MODEL_96f16f4bea9647d6ac5bf94a9b28902b"],"layout":"IPY_MODEL_7d9e2d17c0ff4defac90702c7c60a2e8"}},"e52adff3331b4ee3896ae644b7e8cc99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5d7a615a8be48b88afcdf713dd1592e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6260ec8b3204b8a89bbf09b9ea6f288":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f0d33afd5d5476a8be8db0d633874bd","placeholder":"","style":"IPY_MODEL_de796767110944cfa1393604bfe38234","value":" 938/938 [05:20&lt;00:00,  3.29it/s]"}},"e6e8e9cf09e34880acfaffd61c847faa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8a1770da82c492b89dedc801c3dc475":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11ebaf4b71ed466b81c0e1eb4c5c1268","placeholder":"","style":"IPY_MODEL_6fe333d860394a97b1ecbdf4e413cf46","value":"100%"}},"e8eb2deec7024113b79c9ad0150ff77f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea83cf481eb14048b7ae59487758a9b2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"effcad38fa6b4d63acaa3e9d4d99e09c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f241d0b5c3cf4d2891843a34a5d3158d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f28e4c381bde408c899e876a5d3075c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f49912a08ea7433ba389e920afdee97d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6d1c1f4408847ee83b076b215506964":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d55d466ce22d4bff96085792479b95b8","IPY_MODEL_9b2f5abaa38c4d19981c806614de6b13","IPY_MODEL_dd66f1c489b446c0bf6ed4ba002d4b0a"],"layout":"IPY_MODEL_0d9df55f91944327aee6b42421910096"}},"f6f3f991d60744a5b5ea065ba9f7a1cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f76a3903e9794057adccea363369c42e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8090f13f27c43e4bb2412c5434314a7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8f1912d88ed4198b7925dde37f3ad3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f92b95b91ef74de68b960625d918bc11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1334c3ecc63c42a9bf779fb673f965d5","placeholder":"","style":"IPY_MODEL_05c6f06505e54e829c5afc18f07af47d","value":"Downloading: 100%"}},"faaae34486714069a482d8b7a3dca0c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fac18486d7aa431098228366e845382e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd36d143a46d4c0dae8f46ac7b97fa3a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd745b48adf24f479111281dc891df45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffc7cccd507242d4844c4a7f59e61152":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
